Search.setIndex({"docnames": ["_autosummary/sci_tf", "_autosummary/sci_tf.config", "_autosummary/sci_tf.config.project_config", "_autosummary/sci_tf.config.project_config.Config", "_autosummary/sci_tf.data_handler", "_autosummary/sci_tf.data_handler.data_loader", "_autosummary/sci_tf.data_handler.data_loader.create_tokenizers_dataloaders", "_autosummary/sci_tf.data_handler.data_loader.get_raw_data_opus_books", "_autosummary/sci_tf.data_handler.data_tokenizer", "_autosummary/sci_tf.data_handler.data_tokenizer.check_max_seq_length", "_autosummary/sci_tf.data_handler.data_tokenizer.get_all_text_sequences_from_dataset_in_language", "_autosummary/sci_tf.data_handler.data_tokenizer.get_or_create_tokenizer", "_autosummary/sci_tf.data_handler.masks", "_autosummary/sci_tf.data_handler.masks.causal_mask", "_autosummary/sci_tf.data_handler.two_language_data_set", "_autosummary/sci_tf.data_handler.two_language_data_set.TwoLanguagesDataset", "_autosummary/sci_tf.inference", "_autosummary/sci_tf.inference.tf_inference", "_autosummary/sci_tf.inference.tf_inference.TfInference", "_autosummary/sci_tf.inference.tf_visualizer", "_autosummary/sci_tf.inference.tf_visualizer.TfVisualizer", "_autosummary/sci_tf.model", "_autosummary/sci_tf.model.greedy_decoder", "_autosummary/sci_tf.model.greedy_decoder.GreedyDecoder", "_autosummary/sci_tf.model.layers", "_autosummary/sci_tf.model.layers.FeedForwardBlock", "_autosummary/sci_tf.model.layers.LayerNormalization", "_autosummary/sci_tf.model.layers.MultiHeadAttention", "_autosummary/sci_tf.model.layers.PositionalEncoding", "_autosummary/sci_tf.model.layers.ProjectionLayer", "_autosummary/sci_tf.model.layers.ResidualConnection", "_autosummary/sci_tf.model.layers.TokenEmbeddings", "_autosummary/sci_tf.model.transformer_model", "_autosummary/sci_tf.model.transformer_model.Decoder", "_autosummary/sci_tf.model.transformer_model.DecoderStack", "_autosummary/sci_tf.model.transformer_model.Encoder", "_autosummary/sci_tf.model.transformer_model.EncoderStack", "_autosummary/sci_tf.model.transformer_model.TransformerModel", "_autosummary/sci_tf.trainer", "_autosummary/sci_tf.trainer.transformer_trainer", "_autosummary/sci_tf.trainer.transformer_trainer.TransformerTrainer", "_autosummary/sci_tf.trainer.transformer_validator", "_autosummary/sci_tf.trainer.transformer_validator.TransformerValidator", "_autosummary/sci_tf.utils", "_autosummary/sci_tf.utils.tf_utils", "_autosummary/sci_tf.utils.tf_utils.get_proc_device", "api", "index", "intro", "transformer/embeddings", "transformer/index"], "filenames": ["_autosummary/sci_tf.rst", "_autosummary/sci_tf.config.rst", "_autosummary/sci_tf.config.project_config.rst", "_autosummary/sci_tf.config.project_config.Config.rst", "_autosummary/sci_tf.data_handler.rst", "_autosummary/sci_tf.data_handler.data_loader.rst", "_autosummary/sci_tf.data_handler.data_loader.create_tokenizers_dataloaders.rst", "_autosummary/sci_tf.data_handler.data_loader.get_raw_data_opus_books.rst", "_autosummary/sci_tf.data_handler.data_tokenizer.rst", "_autosummary/sci_tf.data_handler.data_tokenizer.check_max_seq_length.rst", "_autosummary/sci_tf.data_handler.data_tokenizer.get_all_text_sequences_from_dataset_in_language.rst", "_autosummary/sci_tf.data_handler.data_tokenizer.get_or_create_tokenizer.rst", "_autosummary/sci_tf.data_handler.masks.rst", "_autosummary/sci_tf.data_handler.masks.causal_mask.rst", "_autosummary/sci_tf.data_handler.two_language_data_set.rst", "_autosummary/sci_tf.data_handler.two_language_data_set.TwoLanguagesDataset.rst", "_autosummary/sci_tf.inference.rst", "_autosummary/sci_tf.inference.tf_inference.rst", "_autosummary/sci_tf.inference.tf_inference.TfInference.rst", "_autosummary/sci_tf.inference.tf_visualizer.rst", "_autosummary/sci_tf.inference.tf_visualizer.TfVisualizer.rst", "_autosummary/sci_tf.model.rst", "_autosummary/sci_tf.model.greedy_decoder.rst", "_autosummary/sci_tf.model.greedy_decoder.GreedyDecoder.rst", "_autosummary/sci_tf.model.layers.rst", "_autosummary/sci_tf.model.layers.FeedForwardBlock.rst", "_autosummary/sci_tf.model.layers.LayerNormalization.rst", "_autosummary/sci_tf.model.layers.MultiHeadAttention.rst", "_autosummary/sci_tf.model.layers.PositionalEncoding.rst", "_autosummary/sci_tf.model.layers.ProjectionLayer.rst", "_autosummary/sci_tf.model.layers.ResidualConnection.rst", "_autosummary/sci_tf.model.layers.TokenEmbeddings.rst", "_autosummary/sci_tf.model.transformer_model.rst", "_autosummary/sci_tf.model.transformer_model.Decoder.rst", "_autosummary/sci_tf.model.transformer_model.DecoderStack.rst", "_autosummary/sci_tf.model.transformer_model.Encoder.rst", "_autosummary/sci_tf.model.transformer_model.EncoderStack.rst", "_autosummary/sci_tf.model.transformer_model.TransformerModel.rst", "_autosummary/sci_tf.trainer.rst", "_autosummary/sci_tf.trainer.transformer_trainer.rst", "_autosummary/sci_tf.trainer.transformer_trainer.TransformerTrainer.rst", "_autosummary/sci_tf.trainer.transformer_validator.rst", "_autosummary/sci_tf.trainer.transformer_validator.TransformerValidator.rst", "_autosummary/sci_tf.utils.rst", "_autosummary/sci_tf.utils.tf_utils.rst", "_autosummary/sci_tf.utils.tf_utils.get_proc_device.rst", "api.rst", "index.md", "intro.rst", "transformer/embeddings.rst", "transformer/index.md"], "titles": ["sci_tf", "sci_tf.config", "sci_tf.config.project_config", "sci_tf.config.project_config.Config", "sci_tf.data_handler", "sci_tf.data_handler.data_loader", "sci_tf.data_handler.data_loader.create_tokenizers_dataloaders", "sci_tf.data_handler.data_loader.get_raw_data_opus_books", "sci_tf.data_handler.data_tokenizer", "sci_tf.data_handler.data_tokenizer.check_max_seq_length", "sci_tf.data_handler.data_tokenizer.get_all_text_sequences_from_dataset_in_language", "sci_tf.data_handler.data_tokenizer.get_or_create_tokenizer", "sci_tf.data_handler.masks", "sci_tf.data_handler.masks.causal_mask", "sci_tf.data_handler.two_language_data_set", "sci_tf.data_handler.two_language_data_set.TwoLanguagesDataset", "sci_tf.inference", "sci_tf.inference.tf_inference", "sci_tf.inference.tf_inference.TfInference", "sci_tf.inference.tf_visualizer", "sci_tf.inference.tf_visualizer.TfVisualizer", "sci_tf.model", "sci_tf.model.greedy_decoder", "sci_tf.model.greedy_decoder.GreedyDecoder", "sci_tf.model.layers", "sci_tf.model.layers.FeedForwardBlock", "sci_tf.model.layers.LayerNormalization", "sci_tf.model.layers.MultiHeadAttention", "sci_tf.model.layers.PositionalEncoding", "sci_tf.model.layers.ProjectionLayer", "sci_tf.model.layers.ResidualConnection", "sci_tf.model.layers.TokenEmbeddings", "sci_tf.model.transformer_model", "sci_tf.model.transformer_model.Decoder", "sci_tf.model.transformer_model.DecoderStack", "sci_tf.model.transformer_model.Encoder", "sci_tf.model.transformer_model.EncoderStack", "sci_tf.model.transformer_model.TransformerModel", "sci_tf.trainer", "sci_tf.trainer.transformer_trainer", "sci_tf.trainer.transformer_trainer.TransformerTrainer", "sci_tf.trainer.transformer_validator", "sci_tf.trainer.transformer_validator.TransformerValidator", "sci_tf.utils", "sci_tf.utils.tf_utils", "sci_tf.utils.tf_utils.get_proc_device", "&lt;no title&gt;", "How to implement a Transformer ?", "Introduction", "The Embeddings Layer", "Transformer"], "terms": {"document": [0, 47], "src": [], "folder": [], "class": [2, 3, 14, 15, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42], "sourc": [3, 6, 7, 9, 10, 11, 13, 15, 18, 20, 23, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 40, 42, 45], "base": [3, 15, 18, 20, 23, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 40, 42], "object": [3, 18, 20, 23, 40, 42], "configur": [3, 6, 7, 11, 40], "method": [3, 15, 18, 20, 23, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 40, 42], "get_experiments_file_path": 3, "get": [3, 10, 11, 45, 47], "file": 3, "path": 3, "experi": 3, "return": [3, 6, 7, 9, 10, 11, 28, 31, 34, 36, 45], "type": [3, 6, 7, 9, 10, 11, 28, 31, 45], "get_rel_dictionary_file_path": 3, "languag": [3, 9, 10, 11], "construct": 3, "rel": 3, "dictionari": [3, 31], "specif": 3, "paramet": [3, 6, 7, 9, 10, 11, 27, 28, 31, 34, 36, 40], "str": [3, 9, 10, 11, 45], "get_saved_model_file_path": 3, "epoch": 3, "save": 3, "model": [3, 40, 47, 48, 49], "load": [3, 7], "specifi": 3, "function": [5, 8, 12, 44], "config": [6, 7, 11, 18, 20, 37, 40], "creat": [6, 11], "token": [6, 9, 11, 28, 31, 36, 49], "data": [6, 7, 9, 10, 11], "loader": 6, "train": [6, 40], "valid": 6, "target": [6, 31], "vocabulari": 6, "dataload": 6, "raw": [7, 9, 11], "set": [7, 9, 10, 11], "opu": [7, 9, 11], "book": [7, 9, 11], "from": [7, 48], "hug": [7, 11], "face": [7, 11], "dataset": [7, 9, 10, 11, 15], "ds_raw": [9, 10, 11, 15], "check": [9, 30, 47], "maximum": 9, "sequenc": [9, 10, 28, 31], "length": [9, 28, 31], "none": [9, 10], "all": [0, 10, 47, 48, 50], "text": [10, 31], "form": 10, "given": [10, 11], "_description_": 10, "yield": 10, "iter": 10, "over": 10, "everi": [10, 28, 31, 47, 48], "sentenc": 10, "gener": 10, "ani": [10, 28, 31], "provid": [11, 47, 48], "size": [13, 31], "tokenizer_src": 15, "tokenizer_tgt": 15, "src_lang": 15, "tgt_lang": 15, "seq_len": [15, 28], "d_model": [25, 27, 28, 29, 31, 33, 34, 35, 36], "d_ff": [25, 33, 34, 35, 36, 37], "dropout": [25, 30], "modul": [25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37], "attribut": [25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37], "ep": 26, "1e": 26, "06": 26, "http": [0, 26, 31, 47, 48], "www": 26, "pinecon": 26, "io": 26, "learn": [26, 28, 31, 47, 48, 50], "batch": [26, 28, 31], "normal": [26, 30], "num_head": 27, "dropout_prob": [27, 28, 33, 34, 35, 36, 37], "multi": 27, "head": 27, "attent": [0, 27, 47, 48, 50], "int": [27, 28, 31], "number": [27, 28, 31, 49], "embed": [27, 31, 50], "featur": [27, 28, 31, 49], "float": [27, 28], "probabl": [27, 28], "drop": [27, 28], "out": [27, 28, 47], "The": [28, 31, 40, 47, 48, 50], "posit": [28, 31], "encod": [28, 31], "hold": [28, 31], "tensor": [28, 31], "precomput": 28, "valu": 28, "per": [28, 31], "drop_out_prob": 28, "forward": [28, 31, 34, 36], "x": [28, 31, 34, 36], "add": [28, 30], "previou": [28, 30], "which": 28, "usual": 28, "contain": 28, "input": 28, "shape": 28, "i": [0, 28, 31, 45, 47, 48, 50], "dim": [28, 31], "requires_grad_": 28, "fals": 28, "tell": 28, "ar": [28, 31, 50], "A": [0, 28, 31, 47, 48, 50], "appli": [28, 31], "result": 28, "vocab_s": 29, "current": [30, 47, 48], "first": 30, "sublay": 30, "norm": 30, "vice": 30, "versa": 30, "dictionary_s": 31, "fix": 31, "comput": [31, 47], "For": 31, "map": 31, "num_batch": 31, "sequence_length": 31, "perform": [31, 40], "multipli": 31, "sqrt": [31, 49], "scale": [31, 50], "origin": [0, 31, 47, 48, 49, 50], "paper": [0, 31, 49, 50], "doe": 31, "explain": 31, "why": 31, "datasci": 31, "stackexchang": 31, "com": 31, "question": 31, "87906": 31, "transform": [0, 31, 48], "word": 31, "befor": 31, "ad": 31, "todo": 31, "should": 31, "num_stack": [33, 35, 37], "h": [33, 34, 35, 36, 37], "encoder_output": 34, "src_mask": [34, 36], "tgt_mask": 34, "establish": 34, "causal": 34, "mask": 36, "avoid": 36, "actual": 36, "interact": 36, "pad": 36, "src_vocab_s": 37, "tgt_vocab_s": 37, "6": 37, "8": 37, "0": [37, 49], "1": 37, "2048": 37, "handl": 40, "flow": 40, "mt_transform": 40, "perform_train": 40, "processor": 45, "either": 45, "cuda": 45, "cpu": 45, "devic": 45, "string": 45, "architectur": [0, 47, 48, 50], "power": [47, 48], "almost": [47, 48], "state": [47, 48], "art": [47, 48], "deep": [47, 48], "here": [47, 48], "we": [47, 48], "an": [47, 48], "publish": [0, 47, 48], "vaswani": [0, 47, 48, 49, 50], "et": [0, 47, 48, 50], "al": [0, 47, 48, 50], "you": [0, 47, 48, 49, 50], "need": [0, 47, 48, 50], "2017": [0, 47, 48, 50], "arxiv": [0, 47, 48], "org": [0, 47, 48], "ab": [0, 47, 48], "1706": [0, 47, 48], "03762": [0, 47, 48], "would": 47, "like": 47, "us": [47, 49], "other": 47, "your": 47, "own": 47, "solut": [47, 50], "technic": 47, "strateg": 47, "manag": 47, "servic": 47, "artifici": 47, "intellig": 47, "quantum": 47, "contact": 47, "u": 47, "find": 47, "can": [47, 49], "support": 47, "understand": 47, "detail": 47, "code": 47, "refer": [47, 49], "gmbh": 47, "more": 47, "about": 47, "our": 47, "how": [48, 50], "implement": [0, 48, 49], "scratch": 48, "well": 49, "googl": 49, "mention": 49, "d_": 49, "where": 49, "each": 49, "howev": 49, "neither": 49, "nor": 49, "frac": 49, "sum_": 49, "t": 49, "n": 49, "f": 49, "k": 49, "Or": 49, "want": 49, "write": 49, "inlin": 49, "thi": 49, "build": 50, "machin": 50, "translat": 50, "propos": 50, "what": 50, "my": 50, "take": 50}, "objects": {"": [[0, 0, 0, "-", "sci_tf"]], "sci_tf": [[1, 0, 0, "-", "config"], [4, 0, 0, "-", "data_handler"], [16, 0, 0, "-", "inference"], [21, 0, 0, "-", "model"], [38, 0, 0, "-", "trainer"], [43, 0, 0, "-", "utils"]], "sci_tf.config": [[2, 0, 0, "-", "project_config"]], "sci_tf.config.project_config": [[3, 1, 1, "", "Config"]], "sci_tf.config.project_config.Config": [[3, 2, 1, "", "get_experiments_file_path"], [3, 2, 1, "", "get_rel_dictionary_file_path"], [3, 2, 1, "", "get_saved_model_file_path"]], "sci_tf.data_handler": [[5, 0, 0, "-", "data_loader"], [8, 0, 0, "-", "data_tokenizer"], [12, 0, 0, "-", "masks"], [14, 0, 0, "-", "two_language_data_set"]], "sci_tf.data_handler.data_loader": [[6, 3, 1, "", "create_tokenizers_dataloaders"], [7, 3, 1, "", "get_raw_data_opus_books"]], "sci_tf.data_handler.data_tokenizer": [[9, 3, 1, "", "check_max_seq_length"], [10, 3, 1, "", "get_all_text_sequences_from_dataset_in_language"], [11, 3, 1, "", "get_or_create_tokenizer"]], "sci_tf.data_handler.masks": [[13, 3, 1, "", "causal_mask"]], "sci_tf.data_handler.two_language_data_set": [[15, 1, 1, "", "TwoLanguagesDataset"]], "sci_tf.inference": [[17, 0, 0, "-", "tf_inference"], [19, 0, 0, "-", "tf_visualizer"]], "sci_tf.inference.tf_inference": [[18, 1, 1, "", "TfInference"]], "sci_tf.inference.tf_visualizer": [[20, 1, 1, "", "TfVisualizer"]], "sci_tf.model": [[22, 0, 0, "-", "greedy_decoder"], [24, 0, 0, "-", "layers"], [32, 0, 0, "-", "transformer_model"]], "sci_tf.model.greedy_decoder": [[23, 1, 1, "", "GreedyDecoder"]], "sci_tf.model.layers": [[25, 1, 1, "", "FeedForwardBlock"], [26, 1, 1, "", "LayerNormalization"], [27, 1, 1, "", "MultiHeadAttention"], [28, 1, 1, "", "PositionalEncoding"], [29, 1, 1, "", "ProjectionLayer"], [30, 1, 1, "", "ResidualConnection"], [31, 1, 1, "", "TokenEmbeddings"]], "sci_tf.model.layers.PositionalEncoding": [[28, 2, 1, "", "forward"]], "sci_tf.model.layers.TokenEmbeddings": [[31, 2, 1, "", "forward"]], "sci_tf.model.transformer_model": [[33, 1, 1, "", "Decoder"], [34, 1, 1, "", "DecoderStack"], [35, 1, 1, "", "Encoder"], [36, 1, 1, "", "EncoderStack"], [37, 1, 1, "", "TransformerModel"]], "sci_tf.model.transformer_model.DecoderStack": [[34, 2, 1, "", "forward"]], "sci_tf.model.transformer_model.EncoderStack": [[36, 2, 1, "", "forward"]], "sci_tf.trainer": [[39, 0, 0, "-", "transformer_trainer"], [41, 0, 0, "-", "transformer_validator"]], "sci_tf.trainer.transformer_trainer": [[40, 1, 1, "", "TransformerTrainer"]], "sci_tf.trainer.transformer_trainer.TransformerTrainer": [[40, 2, 1, "", "perform_training"]], "sci_tf.trainer.transformer_validator": [[42, 1, 1, "", "TransformerValidator"]], "sci_tf.utils": [[44, 0, 0, "-", "tf_utils"]], "sci_tf.utils.tf_utils": [[45, 3, 1, "", "get_proc_device"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "function", "Python function"]}, "titleterms": {"sci_tf": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], "config": [1, 2, 3], "project_config": [2, 3], "data_handl": [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], "data_load": [5, 6, 7], "create_tokenizers_dataload": 6, "get_raw_data_opus_book": 7, "data_token": [8, 9, 10, 11], "check_max_seq_length": 9, "get_all_text_sequences_from_dataset_in_languag": 10, "get_or_create_token": 11, "mask": [12, 13], "causal_mask": 13, "two_language_data_set": [14, 15], "twolanguagesdataset": 15, "infer": [16, 17, 18, 19, 20], "tf_infer": [17, 18], "tfinfer": 18, "tf_visual": [19, 20], "tfvisual": 20, "model": [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], "greedy_decod": [22, 23], "greedydecod": 23, "layer": [24, 25, 26, 27, 28, 29, 30, 31, 49, 50], "feedforwardblock": 25, "layernorm": 26, "multiheadattent": 27, "positionalencod": 28, "projectionlay": 29, "residualconnect": 30, "tokenembed": 31, "transformer_model": [32, 33, 34, 35, 36, 37], "decod": 33, "decoderstack": 34, "encod": 35, "encoderstack": 36, "transformermodel": 37, "trainer": [38, 39, 40, 41, 42], "transformer_train": [39, 40], "transformertrain": 40, "transformer_valid": [41, 42], "transformervalid": 42, "util": [43, 44, 45], "tf_util": [44, 45], "get_proc_devic": 45, "how": 47, "implement": 47, "transform": [47, 50], "sciform": 47, "ai": 47, "consult": 47, "quick": 47, "select": 47, "introduct": 48, "The": 49, "embed": 49, "what": 49, "ar": 49, "my": 49, "take": 49, "scale": 49, "type": 50}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.viewcode": 1, "nbsphinx": 4, "sphinx": 60}, "alltitles": {"sci_tf.config": [[1, "module-sci_tf.config"]], "sci_tf.config.project_config": [[2, "module-sci_tf.config.project_config"]], "sci_tf.config.project_config.Config": [[3, "sci-tf-config-project-config-config"]], "sci_tf.data_handler": [[4, "module-sci_tf.data_handler"]], "sci_tf.data_handler.data_loader": [[5, "module-sci_tf.data_handler.data_loader"]], "sci_tf.data_handler.data_loader.create_tokenizers_dataloaders": [[6, "sci-tf-data-handler-data-loader-create-tokenizers-dataloaders"]], "sci_tf.data_handler.data_loader.get_raw_data_opus_books": [[7, "sci-tf-data-handler-data-loader-get-raw-data-opus-books"]], "sci_tf.data_handler.data_tokenizer": [[8, "module-sci_tf.data_handler.data_tokenizer"]], "sci_tf.data_handler.data_tokenizer.check_max_seq_length": [[9, "sci-tf-data-handler-data-tokenizer-check-max-seq-length"]], "sci_tf.data_handler.data_tokenizer.get_all_text_sequences_from_dataset_in_language": [[10, "sci-tf-data-handler-data-tokenizer-get-all-text-sequences-from-dataset-in-language"]], "sci_tf.data_handler.data_tokenizer.get_or_create_tokenizer": [[11, "sci-tf-data-handler-data-tokenizer-get-or-create-tokenizer"]], "sci_tf.data_handler.masks": [[12, "module-sci_tf.data_handler.masks"]], "sci_tf.data_handler.masks.causal_mask": [[13, "sci-tf-data-handler-masks-causal-mask"]], "sci_tf.data_handler.two_language_data_set": [[14, "module-sci_tf.data_handler.two_language_data_set"]], "sci_tf.data_handler.two_language_data_set.TwoLanguagesDataset": [[15, "sci-tf-data-handler-two-language-data-set-twolanguagesdataset"]], "sci_tf.inference": [[16, "module-sci_tf.inference"]], "sci_tf.inference.tf_inference": [[17, "module-sci_tf.inference.tf_inference"]], "sci_tf.inference.tf_inference.TfInference": [[18, "sci-tf-inference-tf-inference-tfinference"]], "sci_tf.inference.tf_visualizer": [[19, "module-sci_tf.inference.tf_visualizer"]], "sci_tf.inference.tf_visualizer.TfVisualizer": [[20, "sci-tf-inference-tf-visualizer-tfvisualizer"]], "sci_tf.model": [[21, "module-sci_tf.model"]], "sci_tf.model.greedy_decoder": [[22, "module-sci_tf.model.greedy_decoder"]], "sci_tf.model.greedy_decoder.GreedyDecoder": [[23, "sci-tf-model-greedy-decoder-greedydecoder"]], "sci_tf.model.layers": [[24, "module-sci_tf.model.layers"]], "sci_tf.model.layers.FeedForwardBlock": [[25, "sci-tf-model-layers-feedforwardblock"]], "sci_tf.model.layers.LayerNormalization": [[26, "sci-tf-model-layers-layernormalization"]], "sci_tf.model.layers.MultiHeadAttention": [[27, "sci-tf-model-layers-multiheadattention"]], "sci_tf.model.layers.PositionalEncoding": [[28, "sci-tf-model-layers-positionalencoding"]], "sci_tf.model.layers.ProjectionLayer": [[29, "sci-tf-model-layers-projectionlayer"]], "sci_tf.model.layers.ResidualConnection": [[30, "sci-tf-model-layers-residualconnection"]], "sci_tf.model.layers.TokenEmbeddings": [[31, "sci-tf-model-layers-tokenembeddings"]], "sci_tf.model.transformer_model": [[32, "module-sci_tf.model.transformer_model"]], "sci_tf.model.transformer_model.Decoder": [[33, "sci-tf-model-transformer-model-decoder"]], "sci_tf.model.transformer_model.DecoderStack": [[34, "sci-tf-model-transformer-model-decoderstack"]], "sci_tf.model.transformer_model.Encoder": [[35, "sci-tf-model-transformer-model-encoder"]], "sci_tf.model.transformer_model.EncoderStack": [[36, "sci-tf-model-transformer-model-encoderstack"]], "sci_tf.model.transformer_model.TransformerModel": [[37, "sci-tf-model-transformer-model-transformermodel"]], "sci_tf.trainer": [[38, "module-sci_tf.trainer"]], "sci_tf.trainer.transformer_trainer": [[39, "module-sci_tf.trainer.transformer_trainer"]], "sci_tf.trainer.transformer_trainer.TransformerTrainer": [[40, "sci-tf-trainer-transformer-trainer-transformertrainer"]], "sci_tf.trainer.transformer_validator": [[41, "module-sci_tf.trainer.transformer_validator"]], "sci_tf.trainer.transformer_validator.TransformerValidator": [[42, "sci-tf-trainer-transformer-validator-transformervalidator"]], "sci_tf.utils": [[43, "module-sci_tf.utils"]], "sci_tf.utils.tf_utils": [[44, "module-sci_tf.utils.tf_utils"]], "sci_tf.utils.tf_utils.get_proc_device": [[45, "sci-tf-utils-tf-utils-get-proc-device"]], "How to implement a Transformer ?": [[47, "how-to-implement-a-transformer"]], "Sciform - AI Consulting": [[47, "sciform-ai-consulting"]], "Quick Select": [[47, "quick-select"]], "Introduction": [[48, "introduction"]], "The Embeddings Layer": [[49, "the-embeddings-layer"]], "What are embeddings ?": [[49, "what-are-embeddings"]], "My take on scaling the embeddings": [[49, "my-take-on-scaling-the-embeddings"]], "Transformer": [[50, "transformer"]], "Layer Types": [[50, null]], "sci_tf": [[0, "module-sci_tf"]]}, "indexentries": {"module": [[0, "module-sci_tf"]], "sci_tf": [[0, "module-sci_tf"]]}})